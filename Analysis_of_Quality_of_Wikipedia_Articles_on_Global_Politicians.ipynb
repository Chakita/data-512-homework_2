{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpQOzEQrlI27"
      },
      "source": [
        "# Analysis of Quality of Wikipedia Articles on Global Politicians\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook contains the code for the analysis of quality of Wikipedia articles on global politicians. The goal of the analysis is to find the top 10 countries and regions with the highest coverage (number of articles per capita) and with the most pewr capita high quality articles. Parts of the code were adapted from this [example notebook](https://drive.google.com/file/d/1GN1ULxKombHRzVsNKzj7tBhnBrSWUWXc/view?usp=drive_link) provided by Dr. David McDonald. The Wikipedia Politicians dataset and the Population dataset where also extracted and provided by Dr. David McDonald.\n"
      ],
      "metadata": {
        "id": "bnEDH1gNE7lS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9zcdkgRslI29"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# These are standard python modules\n",
        "import json, time, urllib.parse\n",
        "#\n",
        "# The 'requests' module is not a standard Python module. You will need to install this with pip/pip3 if you do not already have it\n",
        "import requests\n",
        "import pandas as pd\n",
        "import country_converter as coco"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "we need to get region level data from country names for which we will use the country_converter package."
      ],
      "metadata": {
        "id": "SSAgMj_NFIO0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install country_converter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5SQEtey0GS5p",
        "outputId": "3c28f200-66fe-4d0f-d354-f26266bc4b3a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting country_converter\n",
            "  Downloading country_converter-1.2-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.10/dist-packages (from country_converter) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0->country_converter) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0->country_converter) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0->country_converter) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0->country_converter) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0->country_converter) (1.16.0)\n",
            "Downloading country_converter-1.2-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: country_converter\n",
            "Successfully installed country_converter-1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Constants and API request templates\n",
        "We define the constants for our API calls in the following cell. We also include templates for the API requests. Basically we don't expect most of these values to change during the furhter execution of our code and we store them here for convinience. The first two cells are for the ORES API while the third cell contains the constants and request templates for the page info API which is used to extract the latest revison ID which is required by the ORES API"
      ],
      "metadata": {
        "id": "wErz5JttFRDV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "sXa3NhirlI2-"
      },
      "outputs": [],
      "source": [
        "#########\n",
        "#\n",
        "#    CONSTANTS\n",
        "#\n",
        "\n",
        "#    The current LiftWing ORES API endpoint and prediction model\n",
        "#\n",
        "API_ORES_LIFTWING_ENDPOINT = \"https://api.wikimedia.org/service/lw/inference/v1/models/{model_name}:predict\"\n",
        "API_ORES_EN_QUALITY_MODEL = \"enwiki-articlequality\"\n",
        "\n",
        "#\n",
        "#    The throttling rate is a function of the Access token that you are granted when you request the token. The constants\n",
        "#    come from dissecting the token and getting the rate limits from the granted token. An example of that is below.\n",
        "#\n",
        "API_LATENCY_ASSUMED = 0.002       # Assuming roughly 2ms latency on the API and network\n",
        "API_THROTTLE_WAIT = ((60.0*60.0)/5000.0)-API_LATENCY_ASSUMED  # The key authorizes 5000 requests per hour\n",
        "\n",
        "#    When making automated requests we should include something that is unique to the person making the request\n",
        "#    This should include an email - your UW email would be good to put in there\n",
        "#\n",
        "#    Because all LiftWing API requests require some form of authentication, you need to provide your access token\n",
        "#    as part of the header too\n",
        "#\n",
        "REQUEST_HEADER_TEMPLATE = {\n",
        "    'User-Agent': \"<{email_address}>, University of Washington, MSDS DATA 512 - AUTUMN 2024\",\n",
        "    'Content-Type': 'application/json',\n",
        "    'Authorization': \"Bearer {access_token}\"\n",
        "}\n",
        "#\n",
        "#    This is a template for the parameters that we need to supply in the headers of an API request\n",
        "#\n",
        "REQUEST_HEADER_PARAMS_TEMPLATE = {\n",
        "    'email_address' : \"chakim28@uw.edu\",         # your email address should go here\n",
        "    'access_token'  : \"\"          # the access token you create will need to go here\n",
        "}\n",
        "\n",
        "#\n",
        "#    A dictionary of English Wikipedia article titles (keys) and sample revision IDs that can be used for this ORES scoring example\n",
        "#\n",
        "ARTICLE_REVISIONS = { 'Bison':1085687913 , 'Northern flicker':1086582504 , 'Red squirrel':1083787665 , 'Chinook salmon':1085406228 , 'Horseshoe bat':1060601936 }\n",
        "\n",
        "#\n",
        "#    This is a template of the data required as a payload when making a scoring request of the ORES model\n",
        "#\n",
        "ORES_REQUEST_DATA_TEMPLATE = {\n",
        "    \"lang\":        \"en\",     # required that its english - we're scoring English Wikipedia revisions\n",
        "    \"rev_id\":      \"\",       # this request requires a revision id\n",
        "    \"features\":    True\n",
        "}\n",
        "\n",
        "#\n",
        "#    These are used later - defined here so they, at least, have empty values\n",
        "#\n",
        "USERNAME = \"\"\n",
        "ACCESS_TOKEN = \"\"\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZPXQ5RFilI2_"
      },
      "outputs": [],
      "source": [
        "USERNAME = \"\"\n",
        "ACCESS_TOKEN = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0CR8RLGnlBBz"
      },
      "outputs": [],
      "source": [
        "#########\n",
        "#\n",
        "#    CONSTANTS\n",
        "#\n",
        "\n",
        "# The basic English Wikipedia API endpoint\n",
        "API_ENWIKIPEDIA_ENDPOINT = \"https://en.wikipedia.org/w/api.php\"\n",
        "API_HEADER_AGENT = 'User-Agent'\n",
        "\n",
        "# We'll assume that there needs to be some throttling for these requests - we should always be nice to a free data resource\n",
        "API_LATENCY_ASSUMED = 0.002       # Assuming roughly 2ms latency on the API and network\n",
        "API_THROTTLE_WAIT = (1.0/100.0)-API_LATENCY_ASSUMED\n",
        "\n",
        "# When making automated requests we should include something that is unique to the person making the request\n",
        "# This should include an email - your UW email would be good to put in there\n",
        "REQUEST_HEADERS = {\n",
        "    'User-Agent': 'chakim28@uw.edu, University of Washington, MSDS DATA 512 - AUTUMN 2024'\n",
        "}\n",
        "\n",
        "# This is a string of additional page properties that can be returned see the Info documentation for\n",
        "# what can be included. If you don't want any this can simply be the empty string\n",
        "PAGEINFO_EXTENDED_PROPERTIES = \"talkid|url|watched|watchers\"\n",
        "#PAGEINFO_EXTENDED_PROPERTIES = \"\"\n",
        "\n",
        "# This template lists the basic parameters for making this\n",
        "PAGEINFO_PARAMS_TEMPLATE = {\n",
        "    \"action\": \"query\",\n",
        "    \"format\": \"json\",\n",
        "    \"titles\": \"\",           # to simplify this should be a single page title at a time\n",
        "    \"prop\": \"info\",\n",
        "    \"inprop\": PAGEINFO_EXTENDED_PROPERTIES\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions to make API calls\n",
        "\n",
        "This section contains the functions to make API calls to get page info, specifically the latest revision ID for each article and then use this information to get the article quality prediction from the ORES API"
      ],
      "metadata": {
        "id": "SvGaLbBKGdr_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "PFp5G1PjlBB0"
      },
      "outputs": [],
      "source": [
        "#########\n",
        "#\n",
        "#    PROCEDURES/FUNCTIONS\n",
        "#\n",
        "\n",
        "def request_pageinfo_per_article(article_title = None,\n",
        "                                 endpoint_url = API_ENWIKIPEDIA_ENDPOINT,\n",
        "                                 request_template = PAGEINFO_PARAMS_TEMPLATE,\n",
        "                                 headers = REQUEST_HEADERS):\n",
        "\n",
        "    # article title can be as a parameter to the call or in the request_template\n",
        "    if article_title:\n",
        "        request_template['titles'] = article_title\n",
        "\n",
        "    if not request_template['titles']:\n",
        "        raise Exception(\"Must supply an article title to make a pageinfo request.\")\n",
        "\n",
        "    if API_HEADER_AGENT not in headers:\n",
        "        raise Exception(f\"The header data should include a '{API_HEADER_AGENT}' field that contains your UW email address.\")\n",
        "\n",
        "    if 'uwnetid@uw' in headers[API_HEADER_AGENT]:\n",
        "        raise Exception(f\"Use your UW email address in the '{API_HEADER_AGENT}' field.\")\n",
        "\n",
        "    # make the request\n",
        "    try:\n",
        "        # we'll wait first, to make sure we don't exceed the limit in the situation where an exception\n",
        "        # occurs during the request processing - throttling is always a good practice with a free\n",
        "        # data source like Wikipedia - or any other community sources\n",
        "        if API_THROTTLE_WAIT > 0.0:\n",
        "            time.sleep(API_THROTTLE_WAIT)\n",
        "        response = requests.get(endpoint_url, headers=headers, params=request_template)\n",
        "        json_response = response.json()\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        json_response = None\n",
        "    return json_response\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "G8lgL452vA33"
      },
      "outputs": [],
      "source": [
        "def get_latest_revision(json_response):\n",
        "  for page_id in json_response:\n",
        "    latest_revision = json_response[page_id]['lastrevid']\n",
        "  return latest_revision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBTyGbaKlI3B"
      },
      "source": [
        "### Define a function to make the ORES API request\n",
        "\n",
        "The API request will be made using a function to encapsulate call and make access reusable in other notebooks. The procedure is parameterized, relying on the constants above for some important default parameters. The primary assumption is that this function will be used to request data for a set of article revisions. The main parameter is 'article_revid'. One should be able to simply pass in a new article revision id on each call and get back a python dictionary as the result. A valid result will be a dictionary that contains the probabilities that the specific revision is one of six different article quality levels. Generally, quality level with the highest probability score is considered the quality level for the article. This can be tricky when you have two (or more) highly probable quality levels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "c_1_ydu5lI3B"
      },
      "outputs": [],
      "source": [
        "#########\n",
        "#\n",
        "#    PROCEDURES/FUNCTIONS\n",
        "#\n",
        "\n",
        "def request_ores_score_per_article(article_revid = None, email_address=None, access_token=None,\n",
        "                                   endpoint_url = API_ORES_LIFTWING_ENDPOINT,\n",
        "                                   model_name = API_ORES_EN_QUALITY_MODEL,\n",
        "                                   request_data = ORES_REQUEST_DATA_TEMPLATE,\n",
        "                                   header_format = REQUEST_HEADER_TEMPLATE,\n",
        "                                   header_params = REQUEST_HEADER_PARAMS_TEMPLATE):\n",
        "\n",
        "    #    Make sure we have an article revision id, email and token\n",
        "    #    This approach prioritizes the parameters passed in when making the call\n",
        "    if article_revid:\n",
        "        request_data['rev_id'] = article_revid\n",
        "    if email_address:\n",
        "        header_params['email_address'] = email_address\n",
        "    if access_token:\n",
        "        header_params['access_token'] = access_token\n",
        "\n",
        "    #   Making a request requires a revision id - an email address - and the access token\n",
        "    if not request_data['rev_id']:\n",
        "        raise Exception(\"Must provide an article revision id (rev_id) to score articles\")\n",
        "    if not header_params['email_address']:\n",
        "        raise Exception(\"Must provide an 'email_address' value\")\n",
        "    if not header_params['access_token']:\n",
        "        raise Exception(\"Must provide an 'access_token' value\")\n",
        "\n",
        "    # Create the request URL with the specified model parameter - default is a article quality score request\n",
        "    request_url = endpoint_url.format(model_name=model_name)\n",
        "\n",
        "    # Create a compliant request header from the template and the supplied parameters\n",
        "    headers = dict()\n",
        "    for key in header_format.keys():\n",
        "        headers[str(key)] = header_format[key].format(**header_params)\n",
        "\n",
        "    # make the request\n",
        "    try:\n",
        "        # we'll wait first, to make sure we don't exceed the limit in the situation where an exception\n",
        "        # occurs during the request processing - throttling is always a good practice with a free data\n",
        "        # source like ORES - or other community sources\n",
        "        if API_THROTTLE_WAIT > 0.0:\n",
        "            time.sleep(API_THROTTLE_WAIT)\n",
        "        #response = requests.get(request_url, headers=headers)\n",
        "        response = requests.post(request_url, headers=headers, data=json.dumps(request_data))\n",
        "        json_response = response.json()\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        json_response = None\n",
        "    return json_response\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data extraction\n",
        "\n",
        "The following cells contain the code that reads the list of politicians and the corresponding population for each country. It then queries the page info API for the latest revision ID and then queries the ORES API for article quality predicitons. This information is stored in a CSV file named ```wp_politicians_by_country.csv``` which contains the followinf fields:\n",
        "- country\n",
        "- region (this is one of the 22 regions defined by the UN geoscheme)\n",
        "- population\n",
        "- article_title (this is basically the name of the politician)\n",
        "- revision_id (this is ID corresponding to the latest revision of the article)\n",
        "- article_quality (this is the quality predicted by ORES)\n",
        "\n",
        "For some of these articles, we are unable to fetch the latest revision ID thorugh the API and some of the ORES API requests might fail due to timeouts. We keep a track of these erors in ```articles_without_scores.txt```. For the articles that encounterd a timeout issue, we rerun the script for these examples and fill in the data. We aim for a error score of less than 1%.\n",
        "\n",
        "There might also be cases during the merging where either the population dataset does not have an entry for the equivalent Wikipedia country, or vice-versa. We keep track of such countries in ```wp_countries-no_match.txt'`` for future reference.\n"
      ],
      "metadata": {
        "id": "JV_R6CV5G1zl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the CSV files\n",
        "politicians_df = pd.read_csv(\"politicians_by_country_AUG.2024.csv\")\n",
        "population_df = pd.read_csv(\"population_by_country_AUG.2024.csv\")\n",
        "\n",
        "# Rename columns in population_df\n",
        "population_df = population_df.rename(columns={'Geography': 'country', 'Population': 'population'})\n",
        "\n",
        "# Separate regional and country data\n",
        "regional_df = population_df[population_df['country'].str.isupper()]\n",
        "country_population_df = population_df[~population_df['country'].str.isupper()]\n",
        "\n",
        "print(\"Politicians DataFrame columns:\")\n",
        "print(politicians_df.columns)\n",
        "print(\"\\nPopulation DataFrame columns:\")\n",
        "print(population_df.columns)\n",
        "\n",
        "# Create a dictionary to store ORES scores\n",
        "ores_scores = {}\n",
        "\n",
        "# List to store articles without ORES scores\n",
        "articles_without_scores = []\n",
        "\n",
        "# Function to get the latest revision ID\n",
        "def get_latest_revision(json_response):\n",
        "    for page_id in json_response['query']['pages']:\n",
        "        return json_response['query']['pages'][page_id]['lastrevid']\n",
        "\n",
        "# Function to extract the prediction from the flattened ORES response\n",
        "def extract_prediction(ores_response):\n",
        "    # Flatten the dictionary structure to get the prediction\n",
        "    scores = ores_response.get('enwiki', {}).get('scores', {})\n",
        "    for page_id in scores:\n",
        "        prediction = scores[page_id].get('articlequality', {}).get('score', {}).get('prediction')\n",
        "        return prediction\n",
        "\n",
        "# Process each politician in the dataset\n",
        "total_articles = len(politicians_df)\n",
        "for index, row in politicians_df.iterrows():\n",
        "    politician = row['name']\n",
        "\n",
        "    try:\n",
        "        # Get page info\n",
        "        page_info_json = request_pageinfo_per_article(politician)\n",
        "\n",
        "        if 'query' in page_info_json and 'pages' in page_info_json['query']:\n",
        "            latest_revision = get_latest_revision(page_info_json)\n",
        "\n",
        "            # Get ORES score\n",
        "            ores_score = request_ores_score_per_article(latest_revision,\n",
        "                                                        email_address=\"chakim28@uw.edu\",\n",
        "                                                        access_token=ACCESS_TOKEN)\n",
        "\n",
        "            # Extract prediction properly\n",
        "            quality_score = extract_prediction(ores_score)\n",
        "\n",
        "            if quality_score:\n",
        "                ores_scores[politician] = {\n",
        "                    'revision_id': latest_revision,\n",
        "                    'article_quality': quality_score\n",
        "                }\n",
        "            else:\n",
        "                articles_without_scores.append(f\"{politician} (No score in ORES response)\")\n",
        "        else:\n",
        "            articles_without_scores.append(f\"{politician} (No valid page info)\")\n",
        "\n",
        "    except Exception as e:\n",
        "        articles_without_scores.append(f\"{politician} (Error: {str(e)})\")\n",
        "\n",
        "    # Add a delay to avoid hitting rate limits\n",
        "    time.sleep(0.1)\n",
        "\n",
        "# Add ORES scores to politicians_df\n",
        "politicians_df['revision_id'] = politicians_df['name'].map(lambda x: ores_scores.get(x, {}).get('revision_id'))\n",
        "politicians_df['article_quality'] = politicians_df['name'].map(lambda x: ores_scores.get(x, {}).get('article_quality'))\n",
        "\n",
        "# Merge datasets\n",
        "merged_df = pd.merge(politicians_df, country_population_df, on='country', how='outer')\n",
        "\n",
        "# Identify countries with no matches\n",
        "no_match_countries = merged_df[merged_df['name'].isnull() | merged_df['population'].isnull()]['country'].unique()\n",
        "\n",
        "# Save countries with no matches to a text file\n",
        "with open('wp_countries-no_match.txt', 'w') as f:\n",
        "    for country in no_match_countries:\n",
        "        f.write(f\"{country}\\n\")\n",
        "\n",
        "# Create the final CSV file\n",
        "final_df = merged_df.dropna(subset=['name', 'population'])\n",
        "final_df = final_df.rename(columns={'name': 'article_title'})\n",
        "final_df = final_df[['country', 'population', 'article_title', 'revision_id', 'article_quality']]\n",
        "\n",
        "# Add regional data back to the final DataFrame\n",
        "final_df = pd.concat([final_df, regional_df[['country', 'population']]])\n",
        "\n",
        "# Save the final CSV file\n",
        "final_df.to_csv('wp_politicians_by_country.csv', index=False)\n",
        "\n",
        "# Compute and print the score error rate\n",
        "error_rate = len(articles_without_scores) / total_articles\n",
        "print(f\"\\nScore Error Rate: {error_rate:.2%}\")\n",
        "\n",
        "# Print or save the log of articles without scores\n",
        "print(\"\\nArticles without ORES scores:\")\n",
        "for article in articles_without_scores:\n",
        "    print(article)\n",
        "\n",
        "# Optionally, save the log to a file\n",
        "with open('articles_without_scores.txt', 'w') as f:\n",
        "    for article in articles_without_scores:\n",
        "        f.write(f\"{article}\\n\")\n",
        "\n",
        "print(\"\\nProcessing complete. Check wp_countries-no_match.txt, wp_politicians_by_country.csv, and articles_without_scores.txt for results.\")\n"
      ],
      "metadata": {
        "id": "qOGALgW7IehR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We get the list of articles for which the ORES API might have timed out or for which we did not get a valid latest revision ID and we attempt to re-run the API calls on this list. Upon a successful rerun, we update our ```wp_politicians_by_country.csv``` file with the correct information"
      ],
      "metadata": {
        "id": "OCsGo9fpJMal"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of names that encountered errors previously\n",
        "errored_names = [\n",
        "    \"Barbara Eibinger-Miedl\", \"Mehrali Gasimov\", \"Julien Goekint\", \"Kyaw Myint\",\n",
        "    \"André Ngongang Ouandji\", \"Tomás Pimentel\", \"Richard Sumah\", \"Mohamed El Fassi\",\n",
        "    \"Segun ''Aeroland'' Adewale\", \"Binos Dauda Yaroe\", \"Issoufou Saidou-Djermakoye\",\n",
        "    \"Yacouba Sido\", \"José Díaz de Bedoya\", \"João Almeida (politician)\",\n",
        "    \"Álvaro Castello-Branco\", \"Bashir Bililiqo\", \"Gift Banda\"\n",
        "]\n",
        "\n",
        "# Function to get the latest revision ID\n",
        "def get_latest_revision(json_response):\n",
        "    for page_id in json_response['query']['pages']:\n",
        "        return json_response['query']['pages'][page_id]['lastrevid']\n",
        "\n",
        "# Function to extract the prediction from the ORES response\n",
        "def extract_prediction(ores_response):\n",
        "    scores = ores_response.get('enwiki', {}).get('scores', {})\n",
        "    for page_id in scores:\n",
        "        prediction = scores[page_id].get('articlequality', {}).get('score', {}).get('prediction')\n",
        "        return prediction\n",
        "\n",
        "# Dictionary to store results and a list for articles without scores\n",
        "ores_scores = {}\n",
        "articles_without_scores = []\n",
        "\n",
        "# Process each politician in the errored names list\n",
        "for politician in errored_names:\n",
        "    try:\n",
        "        # Get page info to retrieve the latest revision ID\n",
        "        page_info_json = request_pageinfo_per_article(politician)\n",
        "\n",
        "        if 'query' in page_info_json and 'pages' in page_info_json['query']:\n",
        "            latest_revision = get_latest_revision(page_info_json)\n",
        "\n",
        "            # Get ORES score using the latest revision ID\n",
        "            ores_score = request_ores_score_per_article(latest_revision,\n",
        "                                                        email_address=\"chakim28@uw.edu\",\n",
        "                                                        access_token=ACCESS_TOKEN)\n",
        "\n",
        "            # Extract the quality score prediction\n",
        "            quality_score = extract_prediction(ores_score)\n",
        "\n",
        "            if quality_score:\n",
        "                ores_scores[politician] = {\n",
        "                    'revision_id': latest_revision,\n",
        "                    'article_quality': quality_score\n",
        "                }\n",
        "            else:\n",
        "                articles_without_scores.append(f\"{politician} (No score in ORES response)\")\n",
        "        else:\n",
        "            articles_without_scores.append(f\"{politician} (No valid page info)\")\n",
        "\n",
        "    except Exception as e:\n",
        "        articles_without_scores.append(f\"{politician} (Error: {str(e)})\")\n",
        "\n",
        "    # Add a delay to avoid hitting rate limits\n",
        "    time.sleep(0.1)\n",
        "\n",
        "# Print the ORES scores collected\n",
        "print(\"ORES Scores Collected:\")\n",
        "for name, score_data in ores_scores.items():\n",
        "    print(f\"{name}: {score_data}\")\n",
        "\n",
        "# Print or save the log of articles without scores\n",
        "print(\"\\nArticles without ORES scores:\")\n",
        "for article in articles_without_scores:\n",
        "    print(article)\n",
        "\n",
        "# Optionally, save the log to a file\n",
        "with open('articles_without_scores_errored_names.txt', 'w') as f:\n",
        "    for article in articles_without_scores:\n",
        "        f.write(f\"{article}\\n\")\n",
        "\n",
        "print(\"\\nProcessing complete. Check articles_without_scores_errored_names.txt for results.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1f87lALe-tg",
        "outputId": "75249b39-1481-44e7-90d4-2e43fe36aac0"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ORES Scores Collected:\n",
            "Julien Goekint: {'revision_id': 1149315537, 'article_quality': 'Start'}\n",
            "Mohamed El Fassi: {'revision_id': 1190167672, 'article_quality': 'Start'}\n",
            "Binos Dauda Yaroe: {'revision_id': 1226318030, 'article_quality': 'C'}\n",
            "Issoufou Saidou-Djermakoye: {'revision_id': 1177650722, 'article_quality': 'Stub'}\n",
            "Yacouba Sido: {'revision_id': 1177650740, 'article_quality': 'Stub'}\n",
            "José Díaz de Bedoya: {'revision_id': 1216770558, 'article_quality': 'Stub'}\n",
            "João Almeida (politician): {'revision_id': 1219074425, 'article_quality': 'Stub'}\n",
            "Álvaro Castello-Branco: {'revision_id': 1218274868, 'article_quality': 'Stub'}\n",
            "Gift Banda: {'revision_id': 1227050005, 'article_quality': 'Stub'}\n",
            "\n",
            "Articles without ORES scores:\n",
            "Barbara Eibinger-Miedl (Error: 'lastrevid')\n",
            "Mehrali Gasimov (Error: 'lastrevid')\n",
            "Kyaw Myint (Error: 'lastrevid')\n",
            "André Ngongang Ouandji (Error: 'lastrevid')\n",
            "Tomás Pimentel (Error: 'lastrevid')\n",
            "Richard Sumah (Error: 'lastrevid')\n",
            "Segun ''Aeroland'' Adewale (Error: 'lastrevid')\n",
            "Bashir Bililiqo (Error: 'lastrevid')\n",
            "\n",
            "Processing complete. Check articles_without_scores_errored_names.txt for results.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the existing CSV file\n",
        "existing_df = pd.read_csv('wp_politicians_by_country.csv')\n",
        "\n",
        "# New ORES scores that need to be added\n",
        "new_ores_scores = {\n",
        "    \"Julien Goekint\": {'revision_id': 1149315537, 'article_quality': 'Start'},\n",
        "    \"Mohamed El Fassi\": {'revision_id': 1190167672, 'article_quality': 'Start'},\n",
        "    \"Binos Dauda Yaroe\": {'revision_id': 1226318030, 'article_quality': 'C'},\n",
        "    \"Issoufou Saidou-Djermakoye\": {'revision_id': 1177650722, 'article_quality': 'Stub'},\n",
        "    \"Yacouba Sido\": {'revision_id': 1177650740, 'article_quality': 'Stub'},\n",
        "    \"José Díaz de Bedoya\": {'revision_id': 1216770558, 'article_quality': 'Stub'},\n",
        "    \"João Almeida (politician)\": {'revision_id': 1219074425, 'article_quality': 'Stub'},\n",
        "    \"Álvaro Castello-Branco\": {'revision_id': 1218274868, 'article_quality': 'Stub'},\n",
        "    \"Gift Banda\": {'revision_id': 1227050005, 'article_quality': 'Stub'}\n",
        "}\n",
        "\n",
        "# Convert the new ORES scores into a DataFrame\n",
        "new_scores_list = []\n",
        "for name, score_data in new_ores_scores.items():\n",
        "    new_scores_list.append({\n",
        "        'article_title': name,\n",
        "        'revision_id': score_data['revision_id'],\n",
        "        'article_quality': score_data['article_quality']\n",
        "    })\n",
        "\n",
        "new_scores_df = pd.DataFrame(new_scores_list)\n",
        "\n",
        "# Merge the new scores into the existing DataFrame\n",
        "updated_df = existing_df.merge(new_scores_df, on='article_title', how='outer', suffixes=('', '_new'))\n",
        "\n",
        "# Update the original columns with new data if present\n",
        "updated_df['revision_id'] = updated_df['revision_id_new'].combine_first(updated_df['revision_id'])\n",
        "updated_df['article_quality'] = updated_df['article_quality_new'].combine_first(updated_df['article_quality'])\n",
        "\n",
        "# Drop the temporary columns\n",
        "updated_df = updated_df.drop(columns=['revision_id_new', 'article_quality_new'])\n",
        "\n",
        "# Save the updated DataFrame back to the CSV\n",
        "updated_df.to_csv('wp_politicians_by_country.csv', index=False)\n",
        "\n",
        "print(\"Updated ORES scores have been added to wp_politicians_by_country.csv.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6Zl23A7jfGR",
        "outputId": "1fa1cebb-7af3-4015-a56a-9f5d06b5dd9f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated ORES scores have been added to wp_politicians_by_country.csv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mapping countries to regions\n",
        "\n",
        "In this step we attempt to map each country to a region such that each country is mapped to closest (lowest in the hierarchy) region.\n",
        "We use the ```country-converter``` package to get the UN region mapping for each country and map it to the format it is present in our data. Some countries were not mapped succesfully by the package and for these we create a manual country to region mapping."
      ],
      "metadata": {
        "id": "uRkhvX0sJ6HL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the CSV file\n",
        "df = pd.read_csv('wp_politicians_by_country.csv')\n",
        "\n",
        "# Identify rows that are countries (not in all uppercase)\n",
        "df['is_country'] = ~df['country'].str.isupper()\n",
        "\n",
        "# Use country_converter to get regions (UN subregions) only for countries\n",
        "cc = coco.CountryConverter()\n",
        "df.loc[df['is_country'], 'region'] = cc.convert(names=df.loc[df['is_country'], 'country'], to='UNregion')\n",
        "\n",
        "# Manual adjustment to align with your regions\n",
        "region_mapping = {\n",
        "    'Northern Africa': 'NORTHERN AFRICA',\n",
        "    'Western Africa': 'WESTERN AFRICA',\n",
        "    'Eastern Africa': 'EASTERN AFRICA',\n",
        "    'Middle Africa': 'MIDDLE AFRICA',\n",
        "    'Southern Africa': 'SOUTHERN AFRICA',\n",
        "    'Northern America': 'NORTHERN AMERICA',\n",
        "    'Caribbean': 'CARIBBEAN',\n",
        "    'Central America': 'CENTRAL AMERICA',\n",
        "    'South America': 'SOUTH AMERICA',\n",
        "    'Western Asia': 'WESTERN ASIA',\n",
        "    'Central Asia': 'CENTRAL ASIA',\n",
        "    'South Asia': 'SOUTH ASIA',\n",
        "    'Southeast Asia': 'SOUTHEAST ASIA',\n",
        "    'Eastern Asia': 'EAST ASIA',\n",
        "    'Northern Europe': 'NORTHERN EUROPE',\n",
        "    'Western Europe': 'WESTERN EUROPE',\n",
        "    'Eastern Europe': 'EASTERN EUROPE',\n",
        "    'Southern Europe': 'SOUTHERN EUROPE',\n",
        "    'Oceania': 'OCEANIA'\n",
        "}\n",
        "\n",
        "# Custom mapping for countries with missing region information\n",
        "custom_country_mapping = {\n",
        "    'Sri Lanka': 'SOUTH ASIA',\n",
        "    'Indonesia': 'SOUTHEAST ASIA',\n",
        "    'Bangladesh': 'SOUTH ASIA',\n",
        "    'Malaysia': 'SOUTHEAST ASIA',\n",
        "    'Papua New Guinea': 'OCEANIA',\n",
        "    'Iran': 'WESTERN ASIA',\n",
        "    'Pakistan': 'SOUTH ASIA',\n",
        "    'Afghanistan': 'SOUTH ASIA',\n",
        "    'Maldives': 'SOUTH ASIA',\n",
        "    'India': 'SOUTH ASIA',\n",
        "    'Solomon Islands': 'OCEANIA',\n",
        "    'Thailand': 'SOUTHEAST ASIA',\n",
        "    'Timor Leste': 'SOUTHEAST ASIA',\n",
        "    'Marshall Islands': 'OCEANIA',\n",
        "    'Singapore': 'SOUTHEAST ASIA',\n",
        "    'Cambodia': 'SOUTHEAST ASIA',\n",
        "    'Federated States of Micronesia': 'OCEANIA',\n",
        "    'Myanmar': 'SOUTHEAST ASIA',\n",
        "    'Nepal': 'SOUTH ASIA',\n",
        "    'Vietnam': 'SOUTHEAST ASIA',\n",
        "    'Bhutan': 'SOUTH ASIA',\n",
        "    'Vanuatu': 'OCEANIA',\n",
        "    'Tonga': 'OCEANIA',\n",
        "    'Laos': 'SOUTHEAST ASIA',\n",
        "    'Samoa': 'OCEANIA',\n",
        "    'Tuvalu': 'OCEANIA'\n",
        "}\n",
        "\n",
        "# Apply the custom mapping to match your regions (only for country rows)\n",
        "df.loc[df['is_country'], 'region'] = df.loc[df['is_country'], 'region'].map(region_mapping)\n",
        "\n",
        "# Apply the custom country mapping\n",
        "df.loc[df['is_country'], 'region'] = df.loc[df['is_country'], 'region'].fillna(df.loc[df['is_country'], 'country'].map(custom_country_mapping))\n",
        "\n",
        "# For rows that are regions (in all uppercase), set the region to be the same as the country\n",
        "df.loc[~df['is_country'], 'region'] = df.loc[~df['is_country'], 'country']\n",
        "\n",
        "# Remove the temporary 'is_country' column\n",
        "df = df.drop('is_country', axis=1)\n",
        "\n",
        "# Save the updated DataFrame back to CSV\n",
        "df.to_csv('wp_politicians_by_country.csv', index=False)\n",
        "\n",
        "print(\"Updated CSV file saved as 'wp_politicians_by_country.csv'\")\n",
        "\n",
        "# Display the first few rows to verify the new 'region' column\n",
        "print(df[['country', 'region']].head(10))\n",
        "\n",
        "# Count of countries with missing region information\n",
        "missing_regions = df[df['country'] != df['region']]['region'].isna().sum()\n",
        "print(f\"\\nNumber of countries with missing region information: {missing_regions}\")\n",
        "\n",
        "if missing_regions > 0:\n",
        "    print(\"\\nCountries with missing region information:\")\n",
        "    print(df[(df['country'] != df['region']) & df['region'].isna()]['country'].unique())\n",
        "else:\n",
        "    print(\"\\nAll countries have been assigned a region.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGLi1ZBN0bqa",
        "outputId": "9df8121c-2576-42d3-8795-603c5919b8bd"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated CSV file saved as 'wp_politicians_by_country.csv'\n",
            "                    country           region\n",
            "0                      Iraq     WESTERN ASIA\n",
            "1                  Slovenia  SOUTHERN EUROPE\n",
            "2                 Sri Lanka       SOUTH ASIA\n",
            "3                    Guyana    SOUTH AMERICA\n",
            "4                 Indonesia   SOUTHEAST ASIA\n",
            "5                Bangladesh       SOUTH ASIA\n",
            "6                Bangladesh       SOUTH ASIA\n",
            "7                Bangladesh       SOUTH ASIA\n",
            "8                Kazakhstan     CENTRAL ASIA\n",
            "9  Central African Republic    MIDDLE AFRICA\n",
            "\n",
            "Number of countries with missing region information: 0\n",
            "\n",
            "All countries have been assigned a region.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analysis and results\n",
        "In this step we generate tables for the following:\n",
        "- Top 10 countries by coverage: The 10 countries with the highest total articles per capita (in descending order) .\n",
        "- Bottom 10 countries by coverage: The 10 countries with the lowest total articles per capita (in ascending order) .\n",
        "- Top 10 countries by high quality: The 10 countries with the highest high quality articles per capita (in descending order) .\n",
        "- Bottom 10 countries by high quality: The 10 countries with the lowest high quality articles per capita (in ascending order).\n",
        "- Geographic regions by total coverage: A rank ordered list of geographic regions (in descending order) by total articles per capita.\n",
        "- Geographic regions by high quality coverage: Rank ordered list of geographic regions (in descending order) by high quality articles per capita.\n"
      ],
      "metadata": {
        "id": "XFHfP1TwKkzU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file with the added region column\n",
        "df = pd.read_csv('wp_politicians_by_country.csv')\n",
        "\n",
        "# Convert 'population' column to numeric and filter out non-numeric values\n",
        "df['population'] = pd.to_numeric(df['population'], errors='coerce')\n",
        "df = df.dropna(subset=['population'])  # Drop rows where population is NaN\n",
        "\n",
        "# Filter out regions (where 'country' is in all uppercase)\n",
        "df = df[~df['country'].str.isupper()]\n",
        "\n",
        "# Avoid division by zero by removing rows where population is zero\n",
        "df = df[df['population'] > 0]\n",
        "\n",
        "# Determine if an article is high-quality\n",
        "df['is_high_quality'] = df['article_quality'].isin(['FA', 'GA'])\n",
        "\n",
        "# Calculate total and high-quality articles per capita for each country\n",
        "country_stats = df.groupby('country').agg({\n",
        "    'article_title': 'count',\n",
        "    'is_high_quality': 'sum',\n",
        "    'population': 'first',\n",
        "    'region': 'first'\n",
        "}).reset_index()\n",
        "\n",
        "country_stats.columns = ['country', 'total_articles', 'high_quality_articles', 'population', 'region']\n",
        "\n",
        "# Calculate per capita values\n",
        "country_stats['total_articles_per_capita'] = country_stats['total_articles'] / country_stats['population']\n",
        "country_stats['high_quality_articles_per_capita'] = country_stats['high_quality_articles'] / country_stats['population']\n",
        "\n",
        "# Aggregate data by region\n",
        "region_stats = country_stats.groupby('region').agg({\n",
        "    'total_articles_per_capita': 'mean',\n",
        "    'high_quality_articles_per_capita': 'mean'\n",
        "}).reset_index()\n",
        "\n",
        "# Sort regions by total articles per capita\n",
        "regions_by_total_coverage = region_stats.sort_values(by='total_articles_per_capita', ascending=False)\n",
        "\n",
        "# Sort regions by high-quality articles per capita\n",
        "regions_by_high_quality_coverage = region_stats.sort_values(by='high_quality_articles_per_capita', ascending=False)\n",
        "\n",
        "# Print regions by total and high-quality coverage\n",
        "print(\"\\nGeographic regions by total coverage:\")\n",
        "print(regions_by_total_coverage.to_markdown(index=False, floatfmt=\".6f\"))\n",
        "print(\"\\nGeographic regions by high quality coverage:\")\n",
        "print(regions_by_high_quality_coverage.to_markdown(index=False, floatfmt=\".6f\"))\n",
        "\n",
        "# Top 10 countries by coverage\n",
        "top_countries_by_coverage = country_stats.nlargest(10, 'total_articles_per_capita')[['country', 'total_articles_per_capita']]\n",
        "\n",
        "# Bottom 10 countries by coverage\n",
        "bottom_countries_by_coverage = country_stats.nsmallest(10, 'total_articles_per_capita')[['country', 'total_articles_per_capita']]\n",
        "\n",
        "# Top 10 countries by high quality\n",
        "top_countries_by_high_quality = country_stats.nlargest(10, 'high_quality_articles_per_capita')[['country', 'high_quality_articles_per_capita']]\n",
        "\n",
        "# Bottom 10 countries by high quality\n",
        "bottom_countries_by_high_quality = country_stats.nsmallest(10, 'high_quality_articles_per_capita')[['country', 'high_quality_articles_per_capita']]\n",
        "\n",
        "# Print the tables\n",
        "print(\"\\nTop 10 countries by coverage (articles per capita):\")\n",
        "print(top_countries_by_coverage.to_markdown(index=False, floatfmt=\".6f\"))\n",
        "\n",
        "print(\"\\nBottom 10 countries by coverage (articles per capita):\")\n",
        "print(bottom_countries_by_coverage.to_markdown(index=False, floatfmt=\".6f\"))\n",
        "\n",
        "print(\"\\nTop 10 countries by high quality (articles per capita):\")\n",
        "print(top_countries_by_high_quality.to_markdown(index=False, floatfmt=\".6f\"))\n",
        "\n",
        "print(\"\\nBottom 10 countries by high quality (articles per capita):\")\n",
        "print(bottom_countries_by_high_quality.to_markdown(index=False, floatfmt=\".6f\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nL5ooDOj03i3",
        "outputId": "0e9330d7-a70a-44ef-c7e6-2cb43d54b25c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Geographic regions by total coverage:\n",
            "| region          |   total_articles_per_capita |   high_quality_articles_per_capita |\n",
            "|:----------------|----------------------------:|-----------------------------------:|\n",
            "| OCEANIA         |                   62.769424 |                           0.015038 |\n",
            "| CARIBBEAN       |                   51.320891 |                           0.130612 |\n",
            "| SOUTHERN EUROPE |                   15.008149 |                           1.150622 |\n",
            "| SOUTH ASIA      |                   14.694933 |                           0.263691 |\n",
            "| WESTERN EUROPE  |                   11.025725 |                           0.663467 |\n",
            "| NORTHERN EUROPE |                    8.078862 |                           0.408874 |\n",
            "| EASTERN AFRICA  |                    7.565414 |                           0.051389 |\n",
            "| EASTERN EUROPE  |                    6.922870 |                           0.260920 |\n",
            "| CENTRAL AMERICA |                    6.563974 |                           0.212835 |\n",
            "| WESTERN ASIA    |                    5.624691 |                           0.180133 |\n",
            "| SOUTH AMERICA   |                    4.491506 |                           0.181321 |\n",
            "| WESTERN AFRICA  |                    2.968450 |                           0.044332 |\n",
            "| SOUTHERN AFRICA |                    2.951531 |                           0.032949 |\n",
            "| NORTHERN AFRICA |                    2.312342 |                           0.164470 |\n",
            "| SOUTHEAST ASIA  |                    2.117530 |                           0.027192 |\n",
            "| MIDDLE AFRICA   |                    1.955518 |                           0.090397 |\n",
            "| CENTRAL ASIA    |                    1.670533 |                           0.079170 |\n",
            "| EAST ASIA       |                    0.987469 |                           0.075445 |\n",
            "\n",
            "Geographic regions by high quality coverage:\n",
            "| region          |   total_articles_per_capita |   high_quality_articles_per_capita |\n",
            "|:----------------|----------------------------:|-----------------------------------:|\n",
            "| SOUTHERN EUROPE |                   15.008149 |                           1.150622 |\n",
            "| WESTERN EUROPE  |                   11.025725 |                           0.663467 |\n",
            "| NORTHERN EUROPE |                    8.078862 |                           0.408874 |\n",
            "| SOUTH ASIA      |                   14.694933 |                           0.263691 |\n",
            "| EASTERN EUROPE  |                    6.922870 |                           0.260920 |\n",
            "| CENTRAL AMERICA |                    6.563974 |                           0.212835 |\n",
            "| SOUTH AMERICA   |                    4.491506 |                           0.181321 |\n",
            "| WESTERN ASIA    |                    5.624691 |                           0.180133 |\n",
            "| NORTHERN AFRICA |                    2.312342 |                           0.164470 |\n",
            "| CARIBBEAN       |                   51.320891 |                           0.130612 |\n",
            "| MIDDLE AFRICA   |                    1.955518 |                           0.090397 |\n",
            "| CENTRAL ASIA    |                    1.670533 |                           0.079170 |\n",
            "| EAST ASIA       |                    0.987469 |                           0.075445 |\n",
            "| EASTERN AFRICA  |                    7.565414 |                           0.051389 |\n",
            "| WESTERN AFRICA  |                    2.968450 |                           0.044332 |\n",
            "| SOUTHERN AFRICA |                    2.951531 |                           0.032949 |\n",
            "| SOUTHEAST ASIA  |                    2.117530 |                           0.027192 |\n",
            "| OCEANIA         |                   62.769424 |                           0.015038 |\n",
            "\n",
            "Top 10 countries by coverage (articles per capita):\n",
            "| country                        |   total_articles_per_capita |\n",
            "|:-------------------------------|----------------------------:|\n",
            "| Antigua and Barbuda            |                  330.000000 |\n",
            "| Federated States of Micronesia |                  140.000000 |\n",
            "| Marshall Islands               |                  130.000000 |\n",
            "| Tonga                          |                  100.000000 |\n",
            "| Barbados                       |                   83.333333 |\n",
            "| Montenegro                     |                   60.000000 |\n",
            "| Seychelles                     |                   60.000000 |\n",
            "| Bhutan                         |                   55.000000 |\n",
            "| Maldives                       |                   55.000000 |\n",
            "| Samoa                          |                   40.000000 |\n",
            "\n",
            "Bottom 10 countries by coverage (articles per capita):\n",
            "| country       |   total_articles_per_capita |\n",
            "|:--------------|----------------------------:|\n",
            "| China         |                    0.011337 |\n",
            "| India         |                    0.105698 |\n",
            "| Ghana         |                    0.117302 |\n",
            "| Saudi Arabia  |                    0.135501 |\n",
            "| Zambia        |                    0.148515 |\n",
            "| Norway        |                    0.181818 |\n",
            "| Israel        |                    0.204082 |\n",
            "| Egypt         |                    0.304183 |\n",
            "| Cote d'Ivoire |                    0.323625 |\n",
            "| Ethiopia      |                    0.347826 |\n",
            "\n",
            "Top 10 countries by high quality (articles per capita):\n",
            "| country               |   high_quality_articles_per_capita |\n",
            "|:----------------------|-----------------------------------:|\n",
            "| Montenegro            |                           5.000000 |\n",
            "| Luxembourg            |                           2.857143 |\n",
            "| Albania               |                           2.592593 |\n",
            "| Kosovo                |                           2.352941 |\n",
            "| Maldives              |                           1.666667 |\n",
            "| Lithuania             |                           1.379310 |\n",
            "| Croatia               |                           1.315789 |\n",
            "| Guyana                |                           1.250000 |\n",
            "| Palestinian Territory |                           1.090909 |\n",
            "| Slovenia              |                           0.952381 |\n",
            "\n",
            "Bottom 10 countries by high quality (articles per capita):\n",
            "| country             |   high_quality_articles_per_capita |\n",
            "|:--------------------|-----------------------------------:|\n",
            "| Antigua and Barbuda |                           0.000000 |\n",
            "| Bahamas             |                           0.000000 |\n",
            "| Barbados            |                           0.000000 |\n",
            "| Belize              |                           0.000000 |\n",
            "| Benin               |                           0.000000 |\n",
            "| Bhutan              |                           0.000000 |\n",
            "| Botswana            |                           0.000000 |\n",
            "| Cape Verde          |                           0.000000 |\n",
            "| Chad                |                           0.000000 |\n",
            "| China               |                           0.000000 |\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}